\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath,amssymb}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
%\usepackage{subcaption}
\graphicspath{{images/}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}

\title{The User Behavior On Online Judge Website}
\author{yueyi zhuo (2014060843)}
%\institute{School of Mathematical,Sichuan Normal University}
\date{2017}

\begin{document}

\maketitle

\begin{abstract}

In this paper, I develope two types of model to investigate Online Judge website solve record 
and verify some factor holded by order, solver and problem difficulty structure.

\end{abstract}

\section{Introduction}

In 2016 Spring Festival, I solve some problems in Project Euler 
\footnote{In fact, I solve 70+ problems before my interest fade.}, 
which may be most famous online judge 
in Internet except Leetcode. I have noticed the interesting data structure related with rank and difficulty.
But I lacked enough powerful statistic tool to process and model it. 
In fact, I model it as a ordinary least square which is not understood by me in the time. In the naive period,
linear regression model seems like a poor black box model that often be meeted in Machine Learning algorithm.

Now, partly because of study of course of Econometric, I totally understand the random model and the instance,
linear regression. 
Right, there're some random variable constrained by expectation function sharing same parameters, that's all.
A specific linear regression model correspond to many latent models specified indepent variable distribution,
but their expectation function are same. This lead to same result from estimator and model test.

My insight is given by some external user comment about their usage they use OJ. 
Someone claim that thay solve problem by order of the number of solved, but I solve problem by order of ID.
I'm fascinated by thinking that how to identify the percent of two type if we assume only the two type exists.
Since the particular problem represent a more general problem set, I would like to treat it as a bridge to that.

\section{The order model}

Let's formulate my ealiest idea, there're $N$ problems, $n$ solver. Let 
$s_{ij} \in \{ 0,1 \} \quad i \in \{ 1,\dots,n \} \; j \in \{ 1,\dots,N \}$ be whether solver $i$ solve the problem $j$.
So fixing solver $i$, we assume binary random vector $s_i = ( s_{i1} s_{i2} \dots s_{iN} )^T$ 
can be divided into some group which is determined by a specific distribution and they're independent.
Specially, I define the distribution $D_1$ which follow order of ID:


\begin{align*} % Notice align or align* is not used in math mode. They can be directly include in paragraph(normal) mode.
P(s_{i1} = 1) = p \\
P(s_{ij} = 1 | s_{i,j-1}=0) = 0 \\
P(s_{ij} = 1 | s_{i,j-1}=1) = p
\end{align*}


and define $D_2$ which follow order of the number of solved:


\begin{align*}
P(s_{i,r(1)} = 1) &= p \\
P(s_{i,r(j)} = 1 | s_{i,r(j-1)}=0) &= 0 \\
P(s_{i,r(j)} = 1 | s_{i,r(j-1)}=1) &= p
\end{align*}


in which $r(i)$ is function that map the the rank of number of solved to id
(e.g. r(10)=20 means that the rank of number of solved of 20th problem is 10).
Obviously we can replace $r(i)$ to any onther order that mean some interesting thing to get other distribution.

Let $n_1,n_2$ be the individual number that follows $D_1,D_2$, hence $n_1+n_2=n$. Unfortunately, we only can 
observe the sum of every sample sequence:

\[
S_j = \sum_{i=1}^n s_{ij}
\]

We have only data that is once obseration of statistic $S_j$ exactly. It looks like:


\begin{figure}[h]

\centering

\subfigure[Solved number series]{\label{fig:a}\includegraphics[width=0.49\linewidth]{solved-seq1.png}}
\subfigure[Log solved number series]{\label{fig:b}\includegraphics[width=0.49\linewidth]{solved-seq2.png}}
 
\end{figure}

The data is scraped from Project Euler \footnote{Project Euler website: https://projecteuler.net/archives }
, my problem answers, crawler program and basis analysis is hosted on GitHub \footnote{https://github.com/yiyuezhuo/euler }
This is a screenshot for Project Euler website:

\includegraphics[scale=0.55]{euler-oj.png}

Given $n_1,n_2,p$, we can determine a model like we talk above. 
We can specify these values througn minimizing the squard loss function:

\[
\min_{n_1,n_2,p} \sum_{j=1}^N (S^*_j - \E(S_j))^2
\]

In which $S^*_j$ is real value, $S_j$ is random variable defined by model and $n_1,n_2,p$

We would like a model taking more statistic manner instead of only fitting function. 
Thus we assume that the model is \textit{real} 
\footnote{As so often happens, some funny Econometric model claim they're real, 
otherwise statistic model will reduce to awkward explore method.} 
and add a convenient noise term  $\epsilon_j$  to all $S_j$ to let our model can generate(sample) 
any possible sequence \footnote{Anyway, a pure noise model can do it too.}.

\begin{align*}
S_j = \sum_{i=1}^n s_{ij} + \epsilon_j \\
\epsilon_j \sim F \quad i.i.d 
\end{align*}

In which $F$ is a integer distribution that can take binomial distribution or something.

\[
\E(S_j) = \sum_{i=1}^n \E(s_{ij}) + \E(\epsilon_j) = \sum_{i=1}^{n_1} p^j + \sum_{i=n_1+1}^n p^{r(j)}
= n_1 p^j + n_2 p^{r(j)}
\]

and apply least expectation loss rule, very frequentist:

\[
\hat{n_1},\hat{n_2},\hat{p} = \argmin_{n_1,n_2,p} \E\sum_{j=1}^N (S_j - \hat{S_j})^2
\]

In which $\hat{S_j} = \E(S_j | \hat{n_1},\hat{n_2},\hat{p})$.

Unfortunately, this nonlinear form can't be transformed to linear form directly. Temporarily, 
we constrain $n_2 = 0$, then $n_1 = n$, then the $\E(S_j)$ reduce to:

\[
\E{S_j} = n p^j
\]

and

\[
\log \E(S_j) = \log n + j \log(p)
\]

Well, so we can estimate $n,p$ by estimating $\log(n),\log(p)$ by running ordinary least square method to 
minimize the objective function \footnote{Alrough I feel that this conversion is not strict true, since
it assign different weights on every data point.}: 

\[
\min_{\hat{n},\hat{p}} \sum_{j=1}^N ( \log{S_j} - (\log(\hat{n}) + j \log(\hat{p})) )
\]

The OLS model is:

\[
S_j = \log(n) + \log(p) j + \epsilon_j
\]

The result estimated by OLS are:

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &    log(solve)    & \textbf{  R-squared:         } &     0.849   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.849   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     1466.   \\
\textbf{Date:}             & Thu, 11 May 2017 & \textbf{  Prob (F-statistic):} & 7.30e-109   \\
\textbf{Time:}             &     15:04:45     & \textbf{  Log-Likelihood:    } &   -255.98   \\
\textbf{No. Observations:} &         262      & \textbf{  AIC:               } &     516.0   \\
\textbf{Df Residuals:}     &         260      & \textbf{  BIC:               } &     523.1   \\
\textbf{Df Model:}         &           1      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                   & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{Intercept} &      11.4630  &        0.080     &   143.363  &         0.000        &        11.306    11.620       \\
\textbf{id}        &      -0.0202  &        0.001     &   -38.287  &         0.000        &        -0.021    -0.019       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 22.352 & \textbf{  Durbin-Watson:     } &    0.989  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   25.530  \\
\textbf{Skew:}          &  0.712 & \textbf{  Prob(JB):          } & 2.86e-06  \\
\textbf{Kurtosis:}      &  3.558 & \textbf{  Cond. No.          } &     304.  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Does it looks great? We have high $R^2$ and very significance on model and coefficient both. But if we compute 
$\hat{p},\hat{n}$ from  estimated $\log{\hat{p}}, \log{\hat{n}}$ , we got:

\[
\hat{n} = 95132.76 \quad \hat{p} = 0.98002
\]

The $\hat{n}$ looks too small and $\hat{p}$ looks too big as we notice the quick fall of sequence head that
can not be caught by too small $\hat{p}$, as the fit graph illustrate:

\includegraphics[scale=0.55]{simple-fit.png}

Since the fitting is dominated by the tail data, the rift implies that our model is too weak to express 
the mechanism behind the data.

We also can verify this by running a piecewise regression:

\[
S_j = \log(n_1) +  \log(p_1) \mathrm{I}(j<K) j + \log(n_2) + \log(p_2) \mathrm{I}(j \ge K)j + \epsilon_j
\]

The $I(j < K)$ is indicative function, that take $1$ if the statement included in parenthesis is true otherwise take 0.

Obviousily if the model is true, $\hat{p}$ will only change little subject to random noise. 
Below results are obtained by taken first $1/10$ data and last $9/10$ data separately:

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &    log(solve)    & \textbf{  R-squared:         } &     0.887   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.886   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     677.2   \\
\textbf{Date:}             & Thu, 11 May 2017 & \textbf{  Prob (F-statistic):} & 5.95e-122   \\
\textbf{Time:}             &     22:17:31     & \textbf{  Log-Likelihood:    } &   -217.95   \\
\textbf{No. Observations:} &         262      & \textbf{  AIC:               } &     443.9   \\
\textbf{Df Residuals:}     &         258      & \textbf{  BIC:               } &     458.2   \\
\textbf{Df Model:}         &           3      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
             & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{lC}  &      12.9522  &        0.231     &    56.072  &         0.000        &        12.497    13.407       \\
\textbf{lid} &      -0.0741  &        0.016     &    -4.770  &         0.000        &        -0.105    -0.044       \\
\textbf{rC}  &      11.0358  &        0.085     &   130.136  &         0.000        &        10.869    11.203       \\
\textbf{rid} &      -0.0178  &        0.001     &   -33.447  &         0.000        &        -0.019    -0.017       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 27.715 & \textbf{  Durbin-Watson:     } &    1.318  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   38.377  \\
\textbf{Skew:}          &  0.699 & \textbf{  Prob(JB):          } & 4.64e-09  \\
\textbf{Kurtosis:}      &  4.249 & \textbf{  Cond. No.          } & 1.01e+03  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}
 
So we got:
 
\[
\hat{p_1} = 0.9285 \quad \hat{p_2} = 0.9823
\]

Let us show the two fitting figure:

\begin{figure}[h]

\centering

\subfigure[Piecewise regression]{\includegraphics[width=0.49\linewidth]{bi-regress.png}}
\subfigure[Log scale]{\includegraphics[width=0.49\linewidth]{bi-regress-log.png}}
 
\end{figure}

I can't torlerate the odd piecewise regression, although it fit data ok, but it seems to imply that the problem chunks
adapt different mechanism. Perhaps it's true, but the dangrous of overfitting hidden behind can be achieved by 
repeating piecewise process.

Now, let's introduce my initialization idea, how does the two order mix model compare to the single id-order model?

We do the optimization:


\begin{align*}
\min_{n_1,n_2,p} & \quad \E\sum_{j=1}^N (S_j - \hat{S_j})^2 \\
\textrm{s.t.}    & \quad n_1 \in N \\
                 & \quad n_2 \in N \\
                 & \quad p \in [0,1]
\end{align*}

As you can image, I got very unsteady fitting result due to the similarity between the two orders, and it's analogous
to multicolinearity. The data broke my ambition cruely. \footnote{I write the paper and experiment at same time,
I found some result that is beyond my expectations. Since the the value of the paper is very limit and cost is too high
(you see, my English is not very well), I will not pay more time to rewrite something.}

\section{The time series like model}

Considering $S_j$ as time series looks little strange, since obviousily we are dealing a snapshot or cross section 
datathat can be observed many times in principe \footnote{In fact, because data I used is scraped in 2016 
by my early crawler program, I can get another(current time) data to run panel model or otherthing 
if I want to be. Well, but I don't want pay time to work on another boring crawler program, 
although it conflict my early speaking in presentation.}.

We will reduce our scope of discussion into autoregression in this section. Assurming there's a \textit{force}
that is effected by \textit{time}(id order) and difficulty. Since the latter problem solving situation
is only caused by few delay item, the difficulty will cause a permanent shock for solving series.

\subsection{Ground model}

The ground model specification is:

\[
solve_i = \beta_0 + \beta_1 solve_{i-1} + \epsilon_i
\]

The result is:

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &      solve       & \textbf{  R-squared:         } &     0.958   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.958   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     5951.   \\
\textbf{Date:}             & Fri, 12 May 2017 & \textbf{  Prob (F-statistic):} & 1.05e-180   \\
\textbf{Time:}             &     22:16:44     & \textbf{  Log-Likelihood:    } &   -2816.2   \\
\textbf{No. Observations:} &         261      & \textbf{  AIC:               } &     5636.   \\
\textbf{Df Residuals:}     &         259      & \textbf{  BIC:               } &     5644.   \\
\textbf{Df Model:}         &           1      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                   & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{Intercept} &    2004.0414  &      796.786     &     2.515  &         0.013        &       435.037  3573.046       \\
\textbf{$solve\_l1$}  &       0.8586  &        0.011     &    77.142  &         0.000        &         0.837     0.881       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 74.396 & \textbf{  Durbin-Watson:     } &     2.390  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   869.448  \\
\textbf{Skew:}          &  0.739 & \textbf{  Prob(JB):          } & 1.59e-189  \\
\textbf{Kurtosis:}      & 11.818 & \textbf{  Cond. No.          } &  7.82e+04  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

The fitting figure:

\begin{figure}[h]

\centering

\subfigure[Autoregression fitting]{\includegraphics[width=0.49\linewidth]{ar1.png}}
\subfigure[Log scale]{\includegraphics[width=0.49\linewidth]{ar1-log.png}}
 
\end{figure}

The figure looks not very nice since the piecewise pattern occurs again.

\subsection{Detecting effect of difficulty}

Maybe it was putting cart before horse. My initialization idea is to try to design probility model that has more 
realistic meaning and then to simplify it to a OLS or other model. But I found it is easily achived in time series like model,
althougn it has lack of interpretability. \footnote{The mysterious word \textit{force} may lead to a moving 
story that call for deep thought, I prefer to act as crowds standing in front of nude King instead of the honest child.}

I set dummy variable $hard$ that take 1 if the difficulty index of corresponding problem is greater than 30 
otherwise it take 0.

\[
solve_i = \beta_0 + \beta_1 solve_{i-1} + \beta_2 hard \, solve_{i-1} + \epsilon_i
\]

The result is:

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.961   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.961   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     3190.   \\
\textbf{Date:}              & Fri, 12 May 2017 & \textbf{  Prob (F-statistic):} & 1.13e-182   \\
\textbf{Time:}              &     23:06:33     & \textbf{  Log-Likelihood:    } &   -2807.0   \\
\textbf{No. Observations:}  &         261      & \textbf{  AIC:               } &     5620.   \\
\textbf{Df Residuals:}      &         258      & \textbf{  BIC:               } &     5631.   \\
\textbf{Df Model:}          &           2      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{Intercept}          &    3928.8190  &      888.898     &     4.420  &         0.000        &      2178.399  5679.239       \\
\textbf{solve\_l1}           &       0.8508  &        0.011     &    77.956  &         0.000        &         0.829     0.872       \\
\textbf{hard  solve\_l1} &      -0.8225  &        0.189     &    -4.345  &         0.000        &        -1.195    -0.450       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 75.418 & \textbf{  Durbin-Watson:     } &     2.343  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &  1059.431  \\
\textbf{Skew:}          &  0.688 & \textbf{  Prob(JB):          } & 8.86e-231  \\
\textbf{Kurtosis:}      & 12.774 & \textbf{  Cond. No.          } &  9.01e+04  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Does a hard problem decrease "keep rate" from $85.08\%$ to $2.82\%$ actually? Since the first "hard" problem appear
82th location. The fact is hidden in detailed tail figure:

\includegraphics[scale=0.55]{tail.png}

Well, it show a good reason why a standard AR model don't have a intercept term. Let's drop the intercept term and
regress it.

\[
solve_i = \beta_1 solve_{i-1} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &      solve       & \textbf{  R-squared:         } &     0.965   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.965   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     7133.   \\
\textbf{Date:}             & Fri, 12 May 2017 & \textbf{  Prob (F-statistic):} & 5.01e-191   \\
\textbf{Time:}             &     23:44:00     & \textbf{  Log-Likelihood:    } &   -2819.4   \\
\textbf{No. Observations:} &         261      & \textbf{  AIC:               } &     5641.   \\
\textbf{Df Residuals:}     &         260      & \textbf{  BIC:               } &     5644.   \\
\textbf{Df Model:}         &           1      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1} &       0.8698  &        0.010     &    84.458  &         0.000        &         0.850     0.890       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 62.426 & \textbf{  Durbin-Watson:     } &     2.362  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   945.115  \\
\textbf{Skew:}          &  0.399 & \textbf{  Prob(JB):          } & 5.90e-206  \\
\textbf{Kurtosis:}      & 12.288 & \textbf{  Cond. No.          } &      1.00  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}


It seems very like to original model. 

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 hard_i \, solve_{i-1} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.966   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.965   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     3634.   \\
\textbf{Date:}              & Fri, 12 May 2017 & \textbf{  Prob (F-statistic):} & 3.17e-190   \\
\textbf{Time:}              &     23:42:11     & \textbf{  Log-Likelihood:    } &   -2816.6   \\
\textbf{No. Observations:}  &         261      & \textbf{  AIC:               } &     5637.   \\
\textbf{Df Residuals:}      &         259      & \textbf{  BIC:               } &     5644.   \\
\textbf{Df Model:}          &           2      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}           &       0.8713  &        0.010     &    85.207  &         0.000        &         0.851     0.891       \\
\textbf{diff\_hard:solve\_l1} &      -0.4055  &        0.170     &    -2.387  &         0.018        &        -0.740    -0.071       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 61.517 & \textbf{  Durbin-Watson:     } &     2.290  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &  1185.953  \\
\textbf{Skew:}          &  0.215 & \textbf{  Prob(JB):          } & 2.98e-258  \\
\textbf{Kurtosis:}      & 13.434 & \textbf{  Cond. No.          } &      16.6  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

The fitting figure is:

\includegraphics[scale=0.55]{tail2.png}

Ok, this one seems better than the model with intercept. The "hard" problem only decrease "keep rate" 
from $87.13\%$ to $46.57\%$.

\subsection{Dectecting skip effect}

If a solver encounter a hard problem that he can't solve, he may drop the website or wait to time that he can
solve it, that is what above model want to match. But we can consider another behavior in which solver will
try to skip the problem and to solve the next one. If the hard problem is \textit{isolated}, we can expect
the decrease shock caused by the problem will be compressd. For match this, I try to specify the more lag item:

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 hard_i \, solve_{i-1} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.961   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.960   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     2099.   \\
\textbf{Date:}              & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 2.30e-180   \\
\textbf{Time:}              &     09:37:24     & \textbf{  Log-Likelihood:    } &   -2797.0   \\
\textbf{No. Observations:}  &         260      & \textbf{  AIC:               } &     5600.   \\
\textbf{Df Residuals:}      &         257      & \textbf{  BIC:               } &     5611.   \\
\textbf{Df Model:}          &           3      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}           &       0.6957  &        0.060     &    11.564  &         0.000        &         0.577     0.814       \\
\textbf{solve\_l2}           &       0.1718  &        0.053     &     3.230  &         0.001        &         0.067     0.277       \\
\textbf{diff\_hard:solve\_l1} &      -0.3490  &        0.166     &    -2.101  &         0.037        &        -0.676    -0.022       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 76.644 & \textbf{  Durbin-Watson:     } &    2.015  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } & 2966.655  \\
\textbf{Skew:}          & -0.202 & \textbf{  Prob(JB):          } &     0.00  \\
\textbf{Kurtosis:}      & 19.543 & \textbf{  Cond. No.          } &     22.4  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 hard_i \, solve_{i-1} + \beta_4 hard \, solve_{i-2} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.961   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.960   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     1569.   \\
\textbf{Date:}              & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 1.03e-178   \\
\textbf{Time:}              &     09:38:11     & \textbf{  Log-Likelihood:    } &   -2796.8   \\
\textbf{No. Observations:}  &         260      & \textbf{  AIC:               } &     5602.   \\
\textbf{Df Residuals:}      &         256      & \textbf{  BIC:               } &     5616.   \\
\textbf{Df Model:}          &           4      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}           &       0.7014  &        0.061     &    11.406  &         0.000        &         0.580     0.822       \\
\textbf{solve\_l2}           &       0.1667  &        0.054     &     3.064  &         0.002        &         0.060     0.274       \\
\textbf{diff\_hard:solve\_l1} &      -0.4368  &        0.253     &    -1.727  &         0.085        &        -0.935     0.061       \\
\textbf{diff\_hard:solve\_l2} &       0.1235  &        0.268     &     0.461  &         0.645        &        -0.405     0.652       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 75.816 & \textbf{  Durbin-Watson:     } &    2.022  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } & 2893.093  \\
\textbf{Skew:}          & -0.178 & \textbf{  Prob(JB):          } &     0.00  \\
\textbf{Kurtosis:}      & 19.338 & \textbf{  Cond. No.          } &     46.5  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 solve_{i-3} + \beta_4 hard_i \, solve_{i-1} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.968   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.967   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     1910.   \\
\textbf{Date:}              & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 1.06e-188   \\
\textbf{Time:}              &     09:52:21     & \textbf{  Log-Likelihood:    } &   -2745.4   \\
\textbf{No. Observations:}  &         259      & \textbf{  AIC:               } &     5499.   \\
\textbf{Df Residuals:}      &         255      & \textbf{  BIC:               } &     5513.   \\
\textbf{Df Model:}          &           4      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}           &       0.5798  &        0.054     &    10.823  &         0.000        &         0.474     0.685       \\
\textbf{solve\_l2}           &       0.1711  &        0.062     &     2.741  &         0.007        &         0.048     0.294       \\
\textbf{solve\_l3}           &       0.1300  &        0.046     &     2.809  &         0.005        &         0.039     0.221       \\
\textbf{diff\_hard:solve\_l1} &      -0.3202  &        0.142     &    -2.247  &         0.025        &        -0.601    -0.040       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 94.148 & \textbf{  Durbin-Watson:     } &     2.375  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &  1219.969  \\
\textbf{Skew:}          &  1.042 & \textbf{  Prob(JB):          } & 1.22e-265  \\
\textbf{Kurtosis:}      & 13.426 & \textbf{  Cond. No.          } &      26.1  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}


\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 solve_{i-3} + \beta_4 solve_{i-4} + \beta_5 hard_i \, solve_{i-1} + \epsilon_i
\]

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}     &      solve       & \textbf{  R-squared:         } &     0.973   \\
\textbf{Model:}             &       OLS        & \textbf{  Adj. R-squared:    } &     0.972   \\
\textbf{Method:}            &  Least Squares   & \textbf{  F-statistic:       } &     1820.   \\
\textbf{Date:}              & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 4.90e-196   \\
\textbf{Time:}              &     09:52:27     & \textbf{  Log-Likelihood:    } &   -2697.7   \\
\textbf{No. Observations:}  &         258      & \textbf{  AIC:               } &     5405.   \\
\textbf{Df Residuals:}      &         253      & \textbf{  BIC:               } &     5423.   \\
\textbf{Df Model:}          &           5      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}           &       0.3339  &        0.055     &     6.110  &         0.000        &         0.226     0.442       \\
\textbf{solve\_l2}           &       0.2081  &        0.055     &     3.776  &         0.000        &         0.100     0.317       \\
\textbf{solve\_l3}           &       0.0363  &        0.055     &     0.660  &         0.510        &        -0.072     0.145       \\
\textbf{solve\_l4}           &       0.2581  &        0.041     &     6.323  &         0.000        &         0.178     0.338       \\
\textbf{diff\_hard:solve\_l1} &      -0.2193  &        0.124     &    -1.767  &         0.079        &        -0.464     0.025       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 135.383 & \textbf{  Durbin-Watson:     } &    2.179  \\
\textbf{Prob(Omnibus):} &   0.000 & \textbf{  Jarque-Bera (JB):  } & 1713.428  \\
\textbf{Skew:}          &   1.761 & \textbf{  Prob(JB):          } &     0.00  \\
\textbf{Kurtosis:}      &  15.123 & \textbf{  Cond. No.          } &     29.1  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}


To compare the four models, we can see the difficulty shock only exists one period (So solver can't
recall the hell except that is the last problem he try to solve.). And there's significance skip effect, I prefer
to choose the model 3 as real model, hence we can get the average transition diagram
from the estimated parameters to illustrate this:

\[
\begin{array}{c|ccccccc}
\mathrm{Hard} & 1 & 2& 3 & 4 & 5 & 6 &7 \\
\hline
\mathrm{EEEE} & 10000 & 9500 & 9000 & 8593 & 8164 & 7749 & 7386 \\
\mathrm{EEEH} & 10000 & 9500 & 9000 & 8593 & 8164 & 7749 & 4905 \\
\mathrm{EEHE} & 10000 & 9500 & 9000 & 8593 & 8164 & 5135 & 7046 \\
\mathrm{EEHH} & 10000 & 9500 & 9000 & 8593 & 8164 & 5135 & 5402 \\
\mathrm{EHEE} & 10000 & 9500 & 9000 & 8593 & 5413 & 7392 & 6869 \\
\mathrm{EHEH} & 10000 & 9500 & 9000 & 8593 & 5413 & 7392 & 4502 \\
\mathrm{EHHE} & 10000 & 9500 & 9000 & 8593 & 5413 & 5658 & 6644 \\
\mathrm{EHHH} & 10000 & 9500 & 9000 & 8593 & 5413 & 5658 & 4832 \\
\mathrm{HEEE} & 10000 & 9500 & 9000 & 5711 & 7790 & 7208 & 5581 \\
\mathrm{HEEH} & 10000 & 9500 & 9000 & 5711 & 7790 & 7208 & 3273 \\
\mathrm{HEHE} & 10000 & 9500 & 9000 & 5711 & 7790 & 4713 & 5257 \\
\mathrm{HEHH} & 10000 & 9500 & 9000 & 5711 & 7790 & 4713 & 3748 \\
\mathrm{HHEE} & 10000 & 9500 & 9000 & 5711 & 5961 & 6970 & 5237 \\
\mathrm{HHEH} & 10000 & 9500 & 9000 & 5711 & 5961 & 6970 & 3006 \\
\mathrm{HHHE} & 10000 & 9500 & 9000 & 5711 & 5961 & 5061 & 4989 \\
\mathrm{HHHH} & 10000 & 9500 & 9000 & 5711 & 5961 & 5061 & 3369 \\
\end{array}
\]

The $\mathrm{H}$ means that the corresponding problem is "hard", the $\mathrm{E}$ means that the corresponding
problem is not "hard". We can observe the obvious skip effect at all.


\subsection{Dectecting phase effect}

We would like to see the later problem tend to facing buru solver, so the difficulty effect will decrease.

I use dummy variable like above discussion again, even thougn a countinuous decrease tendency is expected,
but I can't see a method that can transform it to a linear model easily.

The model specification is:

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 solve_{i-3} + \beta_4 hard \, solve_{i-1} +
		  \beta_5 hard_i \, right_i \, solve_{i-1} + \epsilon_i
\]

The dummy variable $left_i$ take 1 if the ith problem is in second half part.

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}           &      solve       & \textbf{  R-squared:         } &     0.968   \\
\textbf{Model:}                   &       OLS        & \textbf{  Adj. R-squared:    } &     0.967   \\
\textbf{Method:}                  &  Least Squares   & \textbf{  F-statistic:       } &     1523.   \\
\textbf{Date:}                    & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 4.62e-187   \\
\textbf{Time:}                    &     14:03:17     & \textbf{  Log-Likelihood:    } &   -2745.3   \\
\textbf{No. Observations:}        &         259      & \textbf{  AIC:               } &     5501.   \\
\textbf{Df Residuals:}            &         254      & \textbf{  BIC:               } &     5518.   \\
\textbf{Df Model:}                &           5      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}                 &       0.5800  &        0.054     &    10.808  &         0.000        &         0.474     0.686       \\
\textbf{solve\_l2}                 &       0.1706  &        0.063     &     2.728  &         0.007        &         0.047     0.294       \\
\textbf{solve\_l3}                 &       0.1303  &        0.046     &     2.809  &         0.005        &         0.039     0.222       \\
\textbf{diff\_hard:solve\_l1}       &      -0.3482  &        0.163     &    -2.131  &         0.034        &        -0.670    -0.026       \\
\textbf{diff\_hard:solve\_l1:right} &       0.1162  &        0.329     &     0.353  &         0.725        &        -0.532     0.765       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 94.967 & \textbf{  Durbin-Watson:     } &     2.375  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &  1216.730  \\
\textbf{Skew:}          &  1.059 & \textbf{  Prob(JB):          } & 6.17e-265  \\
\textbf{Kurtosis:}      & 13.405 & \textbf{  Cond. No.          } &      62.1  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

So we can confirm that it is true that solver try to solve latter problem has more steady belief.
The dummy variable method can't be used futher since the condition number has implies the rise of multicolinearity,
thus we get a unsignicant result. But if I replace dummy variable $hard$ to original fake quantity $diff$ variable,
replace $right$ to original order variable $id$, we can get a signicant result:

\[
solve_i = \beta_1 solve_{i-1} + \beta_2 solve_{i-2} + \beta_3 solve_{i-3} + \beta_4 diff \, solve_{i-1} +
		  \beta_5 diff \, id \, solve_{i-1} + \epsilon_i
\]


\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &      solve       & \textbf{  R-squared:         } &     0.970   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.969   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     1619.   \\
\textbf{Date:}             & Sat, 13 May 2017 & \textbf{  Prob (F-statistic):} & 2.34e-190   \\
\textbf{Time:}             &     14:47:52     & \textbf{  Log-Likelihood:    } &   -2737.6   \\
\textbf{No. Observations:} &         259      & \textbf{  AIC:               } &     5485.   \\
\textbf{Df Residuals:}     &         254      & \textbf{  BIC:               } &     5503.   \\
\textbf{Df Model:}         &           5      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
                          & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[95.0\% Conf. Int.]}  \\
\midrule
\textbf{solve\_l1}         &       0.7618  &        0.068     &    11.162  &         0.000        &         0.627     0.896       \\
\textbf{solve\_l2}         &       0.1196  &        0.062     &     1.931  &         0.055        &        -0.002     0.242       \\
\textbf{solve\_l3}         &       0.1470  &        0.045     &     3.245  &         0.001        &         0.058     0.236       \\
\textbf{solve\_l1:diff}    &      -0.0302  &        0.008     &    -3.921  &         0.000        &        -0.045    -0.015       \\
\textbf{solve\_l1:diff:id} &       0.0001  &     5.47e-05     &     2.705  &         0.007        &      4.02e-05     0.000       \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 98.904 & \textbf{  Durbin-Watson:     } &     2.245  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &  1333.652  \\
\textbf{Skew:}          &  1.107 & \textbf{  Prob(JB):          } & 2.52e-290  \\
\textbf{Kurtosis:}      & 13.894 & \textbf{  Cond. No.          } &  4.16e+03  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

This imply a continual decline of effect of difficulty shock.

\section{Conclusion}

I have built a more realistic probility model and fail to extend it as you see. 
Then I develope a indirect time series like model to verify some hypothesis. I can draw conclusion from these survey:

1. Difficulty shock exists significance but it has not memory 

2. The boru solver seems stable more than raw solver when they encounter hard problem.

From result, I can point out that OJ website should place their difficulty problem into proper(latter)
 location to match solver structure to maximize their business and activity.

\end{document}